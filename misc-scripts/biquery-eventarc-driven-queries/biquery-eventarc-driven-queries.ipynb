{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery + Cloud Functions: how to run your queries as soon as a new Google Analytics table is available\n",
    "\n",
    "https://towardsdatascience.com/bigquery-cloud-functions-how-to-run-your-queries-as-soon-as-a-new-google-analytics-table-is-17fbb62f8aaa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup GCP - run `00_setup_env.sh`  - enable APIs, create GCS bucket \n",
    "3. Setup BQ - run `01_setup_bq.sh` - ingest sample data to GCS bucket, create target BQ dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO @justinjm - add creation and setup of service account \n",
    "\n",
    "# # TODO - Set the name of your project\n",
    "# PROJECT_ID=\"your-project-id\" \n",
    "# # TODO - Set the name of your service account\n",
    "# SA_NAME=\"bq-scheduler\" \n",
    "# SA_EMAIL=\"${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \n",
    "\n",
    "# # TODO - set target user that will schedule BQ queries\n",
    "# USER_EMAIL=\"email@company.com\" # can also be group \n",
    "\n",
    "# # Create the service account\n",
    "# gcloud iam service-accounts create $SA_NAME --project $PROJECT_ID\n",
    "\n",
    "# ## service account access --------------------------------------------\n",
    "# ## Grant the service account project editor permissions\n",
    "# ## or `roles/bigquery.jobUser` if minimal required\n",
    "# gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "#   --member \"serviceAccount:${SA_EMAIL}\" \\\n",
    "#   --role \"roles/bigquery.admin\" \\\n",
    "#   --condition=\"None\"\n",
    "\n",
    "# ## user group access --------------------------------------------\n",
    "# gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "#   --member=\"user:${USER_EMAIL}\" \\\n",
    "#   --role=\"roles/bigquery.user\"  \\\n",
    "#   --condition=\"None\"\n",
    "\n",
    "# gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "#   --member=\"user:${USER_EMAIL}\" \\\n",
    "#   --role=\"roles/iam.serviceAccountViewer\" \\\n",
    "#   --condition=\"None\"\n",
    "\n",
    "# ## give users/groups aaccess \n",
    "# ## https://cloud.google.com/iam/docs/service-account-permissions\n",
    "# gcloud iam service-accounts add-iam-policy-binding \"${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n",
    "#   --member=\"user:${USER_EMAIL}\" \\\n",
    "#   --role=\"roles/iam.serviceAccountUser\" \\\n",
    "#   --condition=\"None\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO @justinjm - create scheduled query as part of setup BQ\n",
    "# https://cloud.google.com/bigquery/docs/scheduling-queries#python_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.cloud import bigquery_datatransfer\n",
    "\n",
    "# transfer_client = bigquery_datatransfer.DataTransferServiceClient()\n",
    "\n",
    "# # The project where the query job runs is the same as the project\n",
    "# # containing the destination dataset.\n",
    "# project_id = \"your-project-id\"\n",
    "# dataset_id = \"your_dataset_id\"\n",
    "\n",
    "# # This service account will be used to execute the scheduled queries. Omit\n",
    "# # this request parameter to run the query as the user with the credentials\n",
    "# # associated with this client.\n",
    "# service_account_name = \"abcdef-test-sa@abcdef-test.iam.gserviceaccount.com\"\n",
    "\n",
    "# # Use standard SQL syntax for the query.\n",
    "# query_string = \"\"\"\n",
    "# SELECT * FROM `demos-vertex-ai.bq_eventarc_queries_demo.loan_201` LIMIT 10\n",
    "# \"\"\"\n",
    "\n",
    "# parent = transfer_client.common_project_path(project_id)\n",
    "\n",
    "# transfer_config = bigquery_datatransfer.TransferConfig(\n",
    "#     destination_dataset_id=dataset_id,\n",
    "#     display_name=\"Your Scheduled Query Name\",\n",
    "#     data_source_id=\"scheduled_query\",\n",
    "#     params={\n",
    "#         \"query\": query_string,\n",
    "#         \"destination_table_name_template\": \"your_table_{run_date}\",\n",
    "#         \"write_disposition\": \"WRITE_TRUNCATE\",\n",
    "#         \"partitioning_field\": \"\",\n",
    "#     },\n",
    "#     schedule=\"every 24 hours\",\n",
    "# )\n",
    "\n",
    "# transfer_config = transfer_client.create_transfer_config(\n",
    "#     bigquery_datatransfer.CreateTransferConfigRequest(\n",
    "#         parent=parent,\n",
    "#         transfer_config=transfer_config,\n",
    "#         service_account_name=service_account_name,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# print(\"Created scheduled query '{}'\".format(transfer_config.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - cloud logging filter \n",
    "\n",
    "\n",
    "Demo Version:\n",
    "\n",
    "```txt\n",
    "protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.load.destinationTable.datasetId=\"bq_eventarc_queries_demo\"\n",
    "protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.load.destinationTable.projectId=\"demos-vertex-ai\"\n",
    "protoPayload.methodName=\"jobservice.jobcompleted\"\n",
    "protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.load.destinationTable.tableId:\"loan_201\"\n",
    "```\n",
    "\n",
    "\n",
    "Google Analytics Version: \n",
    "\n",
    "```txt\n",
    "protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.load.destinationTable.datasetId=\"[REPLACE_WITH_YOUR_DATASET_ID]\"\n",
    "protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.load.destinationTable.projectId=\"REPLACE_WITH_YOUR_PROJECT_ID\"\n",
    "protoPayload.authenticationInfo.principalEmail=\"analytics-processing-dev@system.gserviceaccount.com\"\n",
    "protoPayload.methodName=\"jobservice.jobcompleted\"\n",
    "protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.load.destinationTable.tableId:\"ga_sessions\"\n",
    "NOT protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.load.destinationTable.tableId:\"ga_sessions_intraday\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PROJECT_ID=$(gcloud config get-value project)\n",
    "!PROJECT_NUMBER=$(gcloud projects describe $(gcloud config get-value project) --format='value(projectNumber)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Pub/Sub topic\n",
    "!gcloud pubsub topics create bq-load-events-topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create log sink filter based on query above \n",
    "!gcloud logging sinks create bq-load-events-sink \"pubsub.googleapis.com/projects/${PROJECT_ID}/topics/bq-load-events-topic\" \\\n",
    "    --log-filter='protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.load.destinationTable.datasetId=\\\"bq_eventarc_queries_demo\\\" AND protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.load.destinationTable.projectId=\\\"demos-vertex-ai\\\" AND protoPayload.methodName=\\\"jobservice.jobcompleted\\\" AND protoPayload.serviceData.jobCompletedEvent.job.jobConfiguration.load.destinationTable.tableId:\\\"loan_201\\\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grant `serviceAccount:service-PROJECT_NUMBER@gcp-sa-logging.iam.gserviceaccount.com` the Pub/Sub Publisher role on the topic.\n",
    "# More information about sinks can be found at https://cloud.google.com/logging/docs/export/configure_export\n",
    "!gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "  --member=\"serviceAccount:service-${PROJECT_NUMBER}@gcp-sa-logging.iam.gserviceaccount.com\" \\\n",
    "  --role=\"roles/pubsub.publisher\" \\\n",
    "  --project=$PROJECT_ID \\\n",
    "  --condition=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Create Cloud Function \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Cloud Function\n",
    "\n",
    "Create and deploy a Cloud function from the source code in the [functions](functions/) directory:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CF files \n",
    "\n",
    "First we create necessary files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -rf functions/\n",
    "!mkdir functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting functions/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions/main.py \n",
    "import time\n",
    "from google.protobuf.timestamp_pb2 import Timestamp\n",
    "from google.cloud import bigquery_datatransfer_v1\n",
    "\n",
    "def runQuery (parent, requested_run_time):\n",
    "    client = bigquery_datatransfer_v1.DataTransferServiceClient()\n",
    "    projectid = '[enter your projectId here]' # Enter your projectID here\n",
    "    transferid = '[enter your transferId here]'  # Enter your transferId here\n",
    "    parent = client.project_transfer_config_path(projectid, transferid)\n",
    "    start_time = bigquery_datatransfer_v1.types.Timestamp(seconds=int(time.time() + 10))\n",
    "    response = client.start_manual_transfer_runs(parent, requested_run_time=start_time)\n",
    "    print(response)\n",
    "    \n",
    "# do not forget to put google-cloud-bigquery-datatransfer==1 in the requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing functions/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile functions/requirements.txt\n",
    "google-cloud-bigquery-datatransfer==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gcloud functions deploy bq-eventarc-driven-queries-demo \\\n",
    "#   --gen2 \\\n",
    "#   --region=us-central1 \\\n",
    "#   --runtime=python311 \\\n",
    "#   --source=functions/ \\\n",
    "#   --entry-point=run \\\n",
    "#   --trigger-http \\\n",
    "#   --timeout=3600 \\\n",
    "#   --no-allow-unauthenticated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud functions deploy bq-eventarc-driven-queries-demo \\\n",
    "  --gen2 \\\n",
    "  --region=us-central1 \\\n",
    "  --runtime=python311 \\\n",
    "  --source=functions/ \\\n",
    "  --entry-point=run \\\n",
    "  --trigger-topic=YOUR_TOPIC_NAME \\\n",
    "  --timeout=3600 \\\n",
    "  --no-allow-unauthenticated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
